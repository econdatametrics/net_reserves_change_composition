{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6586c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def expand_list_column(\n",
    "    df: pd.DataFrame,\n",
    "    list_col: str,\n",
    "    prefix: str = \"v\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    list_col içindeki listeleri ayrı sütunlara açar.\n",
    "    Örn: [35.18, 14.64, 0] → v1, v2, v3\n",
    "    \"\"\"\n",
    "    values = df[list_col].apply(pd.Series)\n",
    "    values.columns = [f\"{prefix}{i+1}\" for i in range(values.shape[1])]\n",
    "    return pd.concat([df.drop(columns=[list_col]), values], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_lbma_price(\n",
    "    url: str = None,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    price_col_idx: int = 1,           # v1 = USD fiyatı (1. eleman)\n",
    "    list_col: str = \"v\",\n",
    "    date_col: str = \"d\",\n",
    "    value_name: str = \"gold_valuation_price\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    LBMA JSON → tidy DataFrame\n",
    "    - price_col_idx: v1, v2, v3... için 1 tabanlı index\n",
    "    - value_name: çıktı kolon adı\n",
    "    \"\"\"\n",
    "    s = session or requests.Session()\n",
    "    data = s.get(url, params={\"r\": 155972931}).json()\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(data)\n",
    "        .pipe(expand_list_column, list_col=list_col, prefix=\"v\")\n",
    "        .assign(\n",
    "            data_date=lambda d: pd.to_datetime(d[date_col])\n",
    "        )\n",
    "        .rename(\n",
    "            columns={f\"v{price_col_idx}\": value_name}\n",
    "        )\n",
    "        .loc[:, [\"data_date\", value_name]]\n",
    "        .set_index(\"data_date\")\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "\n",
    "def get_download_dir(dirname: str = \"data\") -> Path:\n",
    "    \"\"\"\n",
    "    Mevcut çalışma dizini (Path.cwd()) altındaki data klasörünü hazırlar.\n",
    "    Eski zip/xls dosyalarını siler.\n",
    "    \"\"\"\n",
    "    download_dir = Path.cwd() / dirname\n",
    "    download_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Eski zip/xls temizliği\n",
    "    for f in download_dir.glob(\"*.zip\"):\n",
    "        f.unlink()\n",
    "    for f in download_dir.glob(\"*.xls*\"):\n",
    "        f.unlink()\n",
    "\n",
    "    return download_dir\n",
    "\n",
    "\n",
    "def fetch_latest_urdl_zip(download_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    TCMB URDL sayfasından en güncel haftalık URDL ZIP dosyasını indirir.\n",
    "    \"\"\"\n",
    "    resp = requests.get(TCMB_URDL_URL)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Önce class'a göre ara\n",
    "    zip_tag = soup.select_one(\"a.zip.type-link\")\n",
    "    # Bulamazsa href içinde .zip geçen linki ara\n",
    "    if zip_tag is None:\n",
    "        zip_tag = soup.find(\"a\", href=lambda h: h and \".zip\" in h)\n",
    "    if zip_tag is None:\n",
    "        raise RuntimeError(\"Sayfada .zip linki bulunamadı.\")\n",
    "\n",
    "    zip_url = urljoin(TCMB_URDL_URL, zip_tag[\"href\"])\n",
    "    zip_path = download_dir / \"URDL_latest.zip\"\n",
    "\n",
    "    with requests.get(zip_url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with zip_path.open(\"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    return zip_path\n",
    "\n",
    "\n",
    "def extract_zip(zip_path: Path, extract_dir: Path) -> None:\n",
    "    \"\"\"\n",
    "    ZIP dosyasını verilen klasöre açar.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(extract_dir)\n",
    "\n",
    "\n",
    "def find_urdl_excel(download_dir: Path) -> Path:\n",
    "    \"\"\"\n",
    "    data klasöründe URDL geçen ilk Excel dosyasını bulur.\n",
    "    \"\"\"\n",
    "    excel_files = list(download_dir.glob(\"*URDL*.xls*\"))\n",
    "    if not excel_files:\n",
    "        raise FileNotFoundError(f\"{download_dir} içinde 'URDL' içeren Excel bulunamadı.\")\n",
    "    # Tek olmak zorunda zaten\n",
    "    return excel_files[0]\n",
    "\n",
    "\n",
    "def parse_urdl_excel_to_long(excel_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    URDL Excel'ini tidy (long) formata çevirir:\n",
    "      - boş satır/sütunları temizler\n",
    "      - header satırını programatik seçer (tarihlerin en yoğun olduğu satır)\n",
    "      - ilk kolon 'item', ikinci kolon 'aylık' yapılır\n",
    "      - geniş tablo melt edilerek Tarih-altin_swap_milyar_usd long formatına getirilir\n",
    "    \"\"\"\n",
    "    # Ham okuma, header yok\n",
    "    df = pd.read_excel(excel_path, header=None)\n",
    "\n",
    "    # Tamamen boş satır ve sütunları at\n",
    "    df = df.dropna(how=\"all\", axis=0).dropna(how=\"all\", axis=1).reset_index(drop=True)\n",
    "\n",
    "    # Header satırını programatik bul:\n",
    "    #  - 0. kolonda item isimleri var\n",
    "    #  - 1. kolonda 'Aylık' vb metin var\n",
    "    #  - 2. kolondan itibaren tarihler var\n",
    "    # Bu yüzden sadece 2: sütun aralığına bakıyoruz.\n",
    "    date_like_counts = []\n",
    "    max_check_rows = min(10, len(df))  # ilk 10 satır yeterli\n",
    "    for i in range(max_check_rows):\n",
    "        ser = pd.to_datetime(df.iloc[i, 2:], errors=\"coerce\", dayfirst=True)\n",
    "        date_like_counts.append(ser.notna().sum())\n",
    "\n",
    "    # En çok tarih-like değer içeren satır header olsun\n",
    "    header_row = max(range(max_check_rows), key=lambda i: date_like_counts[i])\n",
    "\n",
    "    header = df.iloc[header_row].copy()\n",
    "    # 2. sütundan sonrası tarih\n",
    "    header.iloc[2:] = pd.to_datetime(\n",
    "        header.iloc[2:],\n",
    "        errors=\"coerce\",\n",
    "        dayfirst=True\n",
    "    ).dt.date\n",
    "\n",
    "    # Dataframe'i header'dan sonrası olarak al\n",
    "    df = df.iloc[header_row + 1:].reset_index(drop=True)\n",
    "    df.columns = header\n",
    "\n",
    "    # Tamamen boş satır kalırsa temizle\n",
    "    df = df.dropna(how=\"all\")\n",
    "\n",
    "    # Kolon isimlerini düzenle: ilk iki kolon label, geri kalanı tarihler\n",
    "    cols = df.columns.to_list()\n",
    "    cols[0] = \"item\"\n",
    "    cols[1] = \"aylık\"\n",
    "    df.columns = cols\n",
    "\n",
    "    # Tidy (long) format\n",
    "    df_long = (\n",
    "        df\n",
    "        .drop(columns=\"aylık\")\n",
    "        .melt(id_vars=\"item\", var_name=\"Tarih\", value_name=\"altin_swap_milyar_usd\")\n",
    "        .dropna(subset=[\"altin_swap_milyar_usd\"])\n",
    "    )\n",
    "\n",
    "    # Tarih kolonunu datetime'a çevir ve index yap\n",
    "    df_long[\"Tarih\"] = pd.to_datetime(df_long[\"Tarih\"])\n",
    "    df_long = df_long.set_index(\"Tarih\").sort_index()\n",
    "\n",
    "    return df_long\n",
    "\n",
    "\n",
    "def get_altin_swap_series(df_urdl_long: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    URDL long dataframe içinden 'II.3. Diğer' satırlarına karşılık gelen\n",
    "    altın swap (milyar USD) serisini döndürür.\n",
    "    \"\"\"\n",
    "    altin_swap = (\n",
    "        df_urdl_long[df_urdl_long[\"item\"] == \"II.3. Diğer\"]\n",
    "        .drop(columns=\"item\")\n",
    "        .rename(columns={\"altin_swap_milyar_usd\": \"altin_swap_milyar_usd\"})\n",
    "    )\n",
    "    return altin_swap\n",
    "\n",
    "\n",
    "def fetch_altin_swap_from_urdl() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    FULL PIPELINE:\n",
    "      - current folder altındaki data/ klasörünü hazırlar\n",
    "      - TCMB URDL sayfasından son haftalık ZIP'i indirir\n",
    "      - ZIP'i data/ altına açar\n",
    "      - URDL Excel'ini bulur\n",
    "      - tidy long dataframe üretir\n",
    "      - 'II.3. Diğer' satırından altın swap serisini döndürür\n",
    "    \"\"\"\n",
    "    download_dir = get_download_dir(dirname=\"data\")\n",
    "    zip_path = fetch_latest_urdl_zip(download_dir)\n",
    "    extract_zip(zip_path, download_dir)\n",
    "    excel_path = find_urdl_excel(download_dir)\n",
    "    df_long = parse_urdl_excel_to_long(excel_path)\n",
    "    altin_swap = get_altin_swap_series(df_long)\n",
    "    return altin_swap\n",
    "\n",
    "\n",
    "def evds_serie_market_all(\n",
    "    series_code: str,\n",
    "    start: str,\n",
    "    end: str,\n",
    "    datagroup: str = \"\",\n",
    "    category: str = \"\",\n",
    "    idx: int = 0,\n",
    "    page_size: int = 20,\n",
    "    aggregation_type: str = \"last\",\n",
    "    frequency: str = \"YEARWEEK\",\n",
    "    date_format_value: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    "    session: Optional[requests.Session] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    s = session or requests.Session()\n",
    "    s.headers.update({\n",
    "        \"Accept\": \"*/*\",\n",
    "        \"Accept-Language\": \"tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"User-Agent\": \"Mozilla/5.0\",\n",
    "        \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "        \"Referer\": \"https://evds2.tcmb.gov.tr/index.php?/evds/serieMarket\",\n",
    "        \"Origin\": \"https://evds2.tcmb.gov.tr\",\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        g = s.get(\"https://evds2.tcmb.gov.tr/index.php?/evds/serieMarket\", timeout=30)\n",
    "        if verbose:\n",
    "            print(\"GET status:\", g.status_code)\n",
    "    except requests.RequestException as e:\n",
    "        if verbose:\n",
    "            print(f\"GET isteğinde hata: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    select_code = f\"{series_code}-{idx}\"\n",
    "    if date_format_value is None:\n",
    "        date_format_value = \"yyyy-ww\"\n",
    "\n",
    "    all_rows = []\n",
    "    total_count = None\n",
    "    skip = 0\n",
    "\n",
    "    while True:\n",
    "        payload = {\n",
    "            \"orderby\": \"Tarih desc\",\n",
    "            \"thousand\": 1,\n",
    "            \"decimal\": 2,\n",
    "            \"sort\": \"Tarih#true\",\n",
    "            \"frequency\": frequency,\n",
    "            \"aggregationType\": aggregation_type,\n",
    "            \"formula\": 0,\n",
    "            \"graphicType\": \"0\",\n",
    "            \"skip\": skip,\n",
    "            \"take\": page_size,\n",
    "            \"select\": select_code,\n",
    "            \"startDate\": start,\n",
    "            \"endDate\": end,\n",
    "            \"categories\": category,\n",
    "            \"mongoAdresses\": \"evds\",\n",
    "            \"datagroupString\": datagroup,\n",
    "            \"obsCountEnabled\": \"\",\n",
    "            \"obsCount\": \"\",\n",
    "            \"userId\": \"\",\n",
    "            \"dateFormatValue\": date_format_value,\n",
    "            \"customFormula\": \"null\",\n",
    "            \"excludedSeries\": \"null\",\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            r = s.post(\"https://evds2.tcmb.gov.tr/EVDSServlet\", data=payload, timeout=60)\n",
    "            if verbose:\n",
    "                print(f\"skip={skip} | status={r.status_code}\")\n",
    "            r.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            if verbose:\n",
    "                print(f\"POST isteğinde hata (skip={skip}): {e}\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except ValueError:\n",
    "            break\n",
    "\n",
    "        if isinstance(data, list) and data:\n",
    "            if total_count is None:\n",
    "                total_count = data[0].get(\"totalCount\", len(data))\n",
    "            items_list = [row.get(\"items\", {}) for row in data]\n",
    "        elif isinstance(data, dict) and \"items\" in data:\n",
    "            if total_count is None:\n",
    "                total_count = data.get(\"totalCount\", len(data[\"items\"]))\n",
    "            items_list = data[\"items\"]\n",
    "        else:\n",
    "            items_list = []\n",
    "\n",
    "        if not items_list:\n",
    "            break\n",
    "\n",
    "        all_rows.extend(items_list)\n",
    "        got = len(items_list)\n",
    "        skip += got\n",
    "\n",
    "        if total_count is not None and len(all_rows) >= total_count:\n",
    "            break\n",
    "\n",
    "    if not all_rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "\n",
    "    # Tarih kolonlarını parse et\n",
    "    for col in df.columns:\n",
    "        cl = col.lower()\n",
    "        if cl.startswith(\"tar\") or cl.startswith(\"date\"):\n",
    "            df[col] = pd.to_datetime(df[col], dayfirst=True, errors=\"coerce\")\n",
    "\n",
    "    # YEARWEEK -> Tarih (Pazartesi)\n",
    "    if \"YEARWEEK\" in df.columns and \"Tarih\" not in df.columns:\n",
    "        def yearweek_to_date(val):\n",
    "            if pd.isna(val):\n",
    "                return pd.NaT\n",
    "            s = str(int(val)).zfill(6)\n",
    "            year = int(s[:4])\n",
    "            week = int(s[4:])\n",
    "            try:\n",
    "                return pd.to_datetime(f\"{year}-W{week}-1\", format=\"%G-W%V-%u\")\n",
    "            except Exception:\n",
    "                return pd.NaT\n",
    "\n",
    "        df[\"Tarih\"] = df[\"YEARWEEK\"].apply(yearweek_to_date)\n",
    "\n",
    "    if \"Tarih\" in df.columns:\n",
    "        df = df.sort_values(\"Tarih\").reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_single_series(\n",
    "    series_code: str,\n",
    "    col_name: str,\n",
    "    start: str,\n",
    "    end: str,\n",
    "    frequency: str,\n",
    "    date_format_value: str,\n",
    "    verbose: bool = False,\n",
    "    session: Optional[requests.Session] = None,\n",
    "    raise_if_empty: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    df = evds_serie_market_all(\n",
    "        series_code=series_code,\n",
    "        start=start,\n",
    "        end=end,\n",
    "        frequency=frequency,\n",
    "        date_format_value=date_format_value,\n",
    "        verbose=verbose,\n",
    "        session=session,\n",
    "    )\n",
    "\n",
    "    if df.empty:\n",
    "        if raise_if_empty:\n",
    "            raise ValueError(f\"{series_code} için veri gelmedi.\")\n",
    "        out = pd.DataFrame(columns=[col_name])\n",
    "        out.index = pd.DatetimeIndex([], name=\"Tarih\")\n",
    "        return out\n",
    "\n",
    "    if \"Tarih\" not in df.columns:\n",
    "        raise ValueError(f\"{series_code} için 'Tarih' yok. Kolonlar: {df.columns.tolist()}\")\n",
    "\n",
    "    non_value_cols = {\"Tarih\", \"YEARWEEK\", \"UNIXTIME\", \"totalCount\"}\n",
    "    value_cols = [c for c in df.columns if c not in non_value_cols]\n",
    "\n",
    "    if len(value_cols) != 1:\n",
    "        raise ValueError(\n",
    "            f\"{series_code} için beklenmeyen kolon yapısı: {df.columns.tolist()}\"\n",
    "        )\n",
    "\n",
    "    val_col = value_cols[0]\n",
    "\n",
    "    out = (\n",
    "        df[[\"Tarih\", val_col]]\n",
    "        .rename(columns={val_col: col_name})\n",
    "        .set_index(\"Tarih\")\n",
    "        .sort_index()\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2) Haftalık resmi altın verileri\n",
    "# =============================================================================\n",
    "\n",
    "SERIES_CONFIG = [\n",
    "    (\"TP.DK.USD.A.YTL\", \"usdtry_weekly\"),\n",
    "    (\"TP.AB.C1\",        \"altin_rezerv_milyar_usd\"),\n",
    "    (\"TP.BL002\",        \"a11\"),\n",
    "    (\"TP.BL0021\",       \"a11_gram\"),\n",
    "    (\"TP.BL0451\",       \"a616_safi_olmayan_gram\"),\n",
    "    (\"TP.BL128\",        \"bl128\"),\n",
    "    (\"TP.BL137\",        \"yurt_ici_mevduat_altin\"),\n",
    "    (\"TP.BL0823\",       \"hazine_altin\"),\n",
    "    (\"TP.BL0891\",       \"zorunlu_karsilik_altin\"),\n",
    "    (\"TP.BL142\",        \"yurt_disi_bankalar_altin\"),\n",
    "    (\"TP.BL1111\",       \"bl111\"),\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_weekly_official_from_evds(\n",
    "    start: str = None ,\n",
    "    end: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Yukarıdaki SERIES_CONFIG listesindeki tüm serileri YEARWEEK frekansta çeker\n",
    "    ve tek DataFrame'de birleştirir.\n",
    "    \"\"\"\n",
    "\n",
    "    if end is None:\n",
    "        end = datetime.today().strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    session = requests.Session()\n",
    "    frequency = \"YEARWEEK\"\n",
    "    date_format_value = \"yyyy-MM-dd\"\n",
    "\n",
    "    common_kwargs = dict(\n",
    "        start=start,\n",
    "        end=end,\n",
    "        frequency=frequency,\n",
    "        date_format_value=date_format_value,\n",
    "        verbose=verbose,\n",
    "        session=session,\n",
    "        raise_if_empty=True,\n",
    "    )\n",
    "\n",
    "    series_dfs = []\n",
    "    for code, name in SERIES_CONFIG:\n",
    "        df_ser = fetch_single_series(\n",
    "            series_code=code,\n",
    "            col_name=name,\n",
    "            **common_kwargs,\n",
    "        )\n",
    "        series_dfs.append(df_ser)\n",
    "\n",
    "    # Hepsini Tarih index'e göre join et\n",
    "    out = series_dfs[0]\n",
    "    for df_ser in series_dfs[1:]:\n",
    "        out = out.join(df_ser, how=\"inner\")\n",
    "\n",
    "    return out.sort_index()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3) Net altın ton-sadece son haftalar\n",
    "# =============================================================================\n",
    "\n",
    "def compute_net_altin_ton(\n",
    "    altin_swap: pd.DataFrame,\n",
    "    start: None,\n",
    "    end: Optional[str] = None,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    EVDS haftalık resmi veriler + URDL'den altın swap serisi ile\n",
    "    Net altın (ton) hesaplar.\n",
    "    \n",
    "    Beklenti:\n",
    "      altin_swap.index : Tarih (haftalık, datetime)\n",
    "      altin_swap.columns: ['altin_swap_milyar_usd']\n",
    "    \"\"\"\n",
    "    df = fetch_weekly_official_from_evds(start= altin_swap.index.min().strftime(\"%d-%m-%Y\") , end=end, verbose=verbose)\n",
    "\n",
    "    # Altın swap ile inner join\n",
    "    df = df.join(altin_swap, how=\"inner\")\n",
    "\n",
    "    # Ara hesaplar (gram cinsinden)\n",
    "    df[\"Brut_altin_varlik\"] = df[\"a11_gram\"]\n",
    "\n",
    "    df[\"Brut_altin_yukumluluk\"] = (\n",
    "        df[\"yurt_ici_mevduat_altin\"]\n",
    "        + df[\"hazine_altin\"]\n",
    "        + df[\"zorunlu_karsilik_altin\"]\n",
    "        + df[\"yurt_disi_bankalar_altin\"]\n",
    "    )\n",
    "\n",
    "    df[\"Brut_altin_swap_gram\"] = (\n",
    "        df[\"a11_gram\"]\n",
    "        * df[\"altin_swap_milyar_usd\"]\n",
    "        / df[\"altin_rezerv_milyar_usd\"]\n",
    "    )\n",
    "\n",
    "    df[\"Net_altin_ton\"] = (\n",
    "        df[\"Brut_altin_varlik\"]\n",
    "        - df[\"Brut_altin_yukumluluk\"]\n",
    "        + df[\"Brut_altin_swap_gram\"]\n",
    "        + df[\"a616_safi_olmayan_gram\"]\n",
    "    ) / 1_000_000  # gram -> ton\n",
    "\n",
    "    return df[[\"Net_altin_ton\"]].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LBMA_GOLD_AM_URL = \"https://prices.lbma.org.uk/json/gold_am.json\"\n",
    "TCMB_URDL_URL = (\n",
    "    \"https://www.tcmb.gov.tr/wps/wcm/connect/tr/tcmb+tr/main+menu/\"\n",
    "    \"istatistikler/odemeler+dengesi+ve+ilgili+istatistikler/\"\n",
    "    \"uluslararasi+rezervler+ve+doviz+likiditesi\"\n",
    ")\n",
    "altin_swap = fetch_altin_swap_from_urdl()\n",
    "\n",
    "df_net_altin = compute_net_altin_ton(\n",
    "    altin_swap=altin_swap,        # URDL'den gelen seri\n",
    "    start= altin_swap.index.min().strftime(\"%d-%m-%Y\")      \n",
    ")\n",
    "\n",
    "\n",
    "last_date = df_net_altin.index.min()\n",
    "gold_price = fetch_lbma_price(LBMA_GOLD_AM_URL)\n",
    "\n",
    "merged = (\n",
    "    gold_price\n",
    "    # tarih kırpma\n",
    "    .loc[lambda df: df.index >= last_date]\n",
    "    .rename_axis(\"Tarih\")\n",
    "    .reset_index()\n",
    "    # net altın ile birleştir\n",
    "    .merge(\n",
    "        df_net_altin.reset_index(),\n",
    "        on=\"Tarih\",\n",
    "        how=\"outer\"\n",
    "    ).pipe(lambda df: df.loc[\n",
    "        (df.index == df.index.max()) | df[\"Net_altin_ton\"].notna()\n",
    "    ])\n",
    "    #.dropna(subset=[\"Net_altin_ton\", \"gold_valuation_price\"])\n",
    "    # USD değer ve ons haftalık değişim\n",
    "    .assign(\n",
    "        net_altin_usd=lambda d:\n",
    "            d[\"Net_altin_ton\"] * 32.1507466 * d[\"gold_valuation_price\"] / 1_000_000,\n",
    "        ons_price_change_weekly=lambda d:\n",
    "            d[\"gold_valuation_price\"].pct_change()\n",
    "    )\n",
    "    # değer değişim etkisi (bir önceki haftanın stok * haftalık fiyat değişimi)\n",
    "    .assign(\n",
    "        gold_valuation=lambda d:\n",
    "            d[\"net_altin_usd\"].shift(1) * d[\"ons_price_change_weekly\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "merged.set_index(\"Tarih\").to_excel(\"results/gold_valuation.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dc3abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
